{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research metod testowych dla drzew decyzyjnych i regresji\n",
    "\n",
    "## 1.1 Metryki dla klasyfikacji (drzewa decyzyjne)\n",
    "\n",
    "1. **Accuracy (dokładność)**  \n",
    "   Procent poprawnie sklasyfikowanych próbek.  \n",
    "\n",
    "    $\\text{Accuracy} = \\frac{\\text{Liczba poprawnych predykcji}}{\\text{Wszystkie próbki}}$\n",
    "\n",
    "\n",
    "2. **Precision (precyzja)**  \n",
    "   Liczba prawidłowych pozytywnych wyników podzielona przez wszystkie wyniki sklasyfikowane jako pozytywne. Jest to część prawidłowych pozytywnych przewidywań wśród wszystkich przewidywań pozytywnych.\n",
    "\n",
    "    $\\text{Precision} = \\frac{TP}{TP + FP}$\n",
    "\n",
    "\n",
    "3. **Recall (czułość, pokrycie)**  \n",
    "   Liczba prawidłowych pozytywnych wyników podzielona przez wszystkie rzeczywiste pozytywne przypadki. Odsetek prawdziwych pozytywnych przypadków, które model poprawnie rozpoznał jako pozytywne.\n",
    "\n",
    "    $\\text{Recall} = \\frac{TP}{TP + FN}$\n",
    "\n",
    "\n",
    "4. **F1-Score**  \n",
    "   Harmoniczna średnia precyzji i czułości. **F1** służy do zbalansowanej oceny modeli w sytuacjach, gdy ważne jest jednoczesne utrzymanie wysokiej precyzji i czułości.\n",
    "\n",
    "    $F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
    "\n",
    "\n",
    "5. **ROC-AUC (Area Under Curve)**  \n",
    "   Pole pod krzywą ROC (Receiver Operating Characteristic), mierzy zdolność modelu do rozróżniania klas.\n",
    "\n",
    "\n",
    "6. **Log Loss (logarytmiczne straty)**  \n",
    "   Miara penalizująca modele za przewidywania z niską pewnością (im wyższa niepewność lub nietrafność, tym wyższa kara).\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Metryki dla regresji\n",
    "\n",
    "1. **Mean Absolute Error (MAE)**  \n",
    "   Średnia wartość bezwzględnych różnic między przewidywanymi a rzeczywistymi wartościami. Wskazuje, o ile (średnio) „pudłujemy” w jednostkach wyjściowych (np. zł, USD, km/h). Na przykład: MAE = 1000 zł oznacza, że model pomyli się średnio o 1000 zł względem faktycznej ceny czy wartości.\n",
    "\n",
    "    $MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$\n",
    "\n",
    "\n",
    "2. **Mean Squared Error (MSE)**  \n",
    "   Średnia kwadratów różnic między przewidywanymi a rzeczywistymi wartościami. MSE zwiększa „karę” za duże błędy, ponieważ każda różnica jest podnoszona do kwadratu, co pomaga wychwycić sytuacje, w których model znacząco się myli.\n",
    "\n",
    "    $MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE)**  \n",
    "   Pierwiastek kwadratowy z MSE. Wynik jest w tych samych jednostkach co dane wyjściowe (np. dolary, złote czy kilogramy), co często ułatwia interpretację oraz wskazuje, o ile (średnio) model „pudłuje” w przewidywaniach.  \n",
    "\n",
    "    $RMSE = \\sqrt{MSE}$\n",
    "\n",
    "\n",
    "4. **R-squared (R²)**  \n",
    "   Informuje o tym, jaka część zmienności (wariancji) zmiennej objaśnianej w próbie pokrywa się z korelacjami ze zmiennymi zawartymi w modelu. Jest on więc miarą stopnia, w jakim model pasuje do próby. Dopasowanie modelu jest tym lepsze, im wartość R² jest bliższa jedności.\n",
    "\n",
    "    $R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}$\n",
    "\n",
    "\n",
    "5. **Mean Absolute Percentage Error (MAPE)**  \n",
    "   Średnia wartość procentowych różnic między przewidywaniami a wartościami rzeczywistymi.Relatywna (względna) wielkość błędu.\n",
    "\n",
    "    $MAPE = \\frac{100\\%}{n} \\sum_{i=1}^{n} \\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Jak będzie wyglądał proces testowy?\n",
    "\n",
    "1. **Podział danych**  \n",
    "Podzielimy dane na zbiór treningowy (70-80%) i testowy (20-30%). Dodatkowo można wykorzystać walidację krzyżową (k-fold cross-validation), żeby uzyskać bardziej wiarygodny wynik.\n",
    "\n",
    "2. **Trening modelu**  \n",
    "Wytrenujemy drzewo decyzyjne na danych treningowych.  W przypadku regresji, wytrenujemy odpowiedni model (np. regresję liniową, drzewo regresyjne, itp.).\n",
    "\n",
    "3. **Predykcja**  \n",
    "Użyjemy danych testowych, by wygenerować przewidywane wartości (regresja) lub klasy (klasyfikacja).\n",
    "\n",
    "4. **Ocena modelu** \n",
    "- Obliczamy MSE, RMSE, MAE, MAPE, R².\n",
    "\n",
    "- Porównujemy te wartości dla każdej modeli. Musimy zobaczyć który model osiąga najlepsze wyniki na wybranej metryce. Metryki na których nam zależy to MAE i MAPE.\n",
    "\n",
    "- Oceniamy, czy wielkość błędu (np. RMSE = 5000 zł) jest akceptowalna w danym kontekście. Czy różnice między modelami są istotne praktycznie?\n",
    "\n",
    "- Analizujemy, jak rozkładają się błędy (reszty) w funkcji wartości rzeczywistych. Jeśli w danych są wzorce (np. rosnący błąd dla wyższych cen), musimy zastanowić się nad korektą modelu lub cech.\n",
    "\n",
    "- Wizualizacja.\n",
    "1.  Wykres rzeczywiste vs. przewidywane (Actual vs. Predicted)\n",
    "    - Oś X: wartości przewidywane przez model.  \n",
    "    - Oś Y: rzeczywiste wartości.  \n",
    "    - Jeśli model jest idealny, punkty powinny leżeć wzdłuż przekątnej (linii $ y = x $).\n",
    "2. Wykres reszt (Residual Plot)\n",
    "   - Oś X: wartości przewidywane.  \n",
    "   - Oś Y: reszty (różnice: $ y_{\\text{rzeczywiste}} - y_{\\text{przewidywane}} $).  \n",
    "   - Pozwala sprawdzić, czy błędy rozkładają się losowo (idealnie powinny oscylować wokół zera).\n",
    "\n",
    "5. **Walidacja krzyżowa** (Aby uniknąć problemu z **przetrenowaniem (overfitting)**)\n",
    "- Zbiór danych jest dzielimy na $ k $ równych części (zwanych \"foldami\").  Każdy \"fold\" to podzbiór danych, który jest używany zarówno do treningu, jak i do testowania modelu, ale nie jednocześnie.\n",
    "\n",
    "- W każdej iteracji model jest trenowany na $ k-1 $ foldach, a testowany na pozostałym jednym foldzie.  Proces ten powtarza się $ k $ razy, czyli każda część zbioru danych pełni rolę testową raz.\n",
    "\n",
    "- Po każdej iteracji obliczane są metryki modelu (np. RMSE, MAE). Na koniec wszystkie wyniki są uśredniane, co daje dokładniejszą ocenę efektywności modelu niż w przypadku pojedynczego podziału treningowego/testowego.\n",
    "\n",
    "**Zalety K-Fold Cross-Validation:**  \n",
    "   - **Lepsza ocena uogólnienia**: Ponieważ model jest testowany na różnych częściach danych, daje to lepszy obraz jego zdolności do generalizacji (sprawdzania na nowych, niewidzianych danych).\n",
    "   - **Zmniejszenie ryzyka przetrenowania**: Użycie różnych foldów do testowania sprawia, że model nie jest trenowany na tym samym zestawie danych, co pozwala na lepszą ocenę jego rzeczywistej wydajności.\n",
    "   - **Większa stabilność wyników**: Uśrednianie wyników z różnych foldów minimalizuje wpływ przypadkowych fluktuacji wyników w zależności od konkretnego podziału danych.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
