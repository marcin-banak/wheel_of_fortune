{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Methods for Decision Trees and Regression\n",
    "\n",
    "## **Metrics**\n",
    "\n",
    "### 1.1 Metrics for Classification (Decision Trees)\n",
    "\n",
    "1. **Accuracy**  \n",
    "   The percentage of correctly classified samples.\n",
    "\n",
    "    $\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Samples}}$\n",
    "\n",
    "\n",
    "2. **Precision**  \n",
    "   The number of true positive results divided by all results predicted as positive. It measures the proportion of correctly predicted positive outcomes out of all predicted positive outcomes.\n",
    "\n",
    "    $\\text{Precision} = \\frac{TP}{TP + FP}$\n",
    "\n",
    "\n",
    "3. **Recall (Sensitivity, Coverage)**  \n",
    "   The number of true positive results divided by all actual positive cases. It measures the proportion of true positive cases correctly identified by the model.\n",
    "\n",
    "    $\\text{Recall} = \\frac{TP}{TP + FN}$\n",
    "\n",
    "\n",
    "4. **F1-Score**  \n",
    "   The harmonic mean of precision and recall. **F1** is used for balanced evaluation of models in situations where maintaining both high precision and recall is important.\n",
    "\n",
    "    $F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
    "\n",
    "\n",
    "5. **ROC-AUC (Area Under Curve)**  \n",
    "   The area under the Receiver Operating Characteristic curve, measuring the model's ability to distinguish between classes.\n",
    "\n",
    "\n",
    "6. **Log Loss**  \n",
    "   A metric penalizing models for predictions with low confidence. The higher the uncertainty or inaccuracy, the higher the penalty.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Metrics for Regression\n",
    "\n",
    "1. **Mean Absolute Error (MAE)**  \n",
    "   The average of absolute differences between predicted and actual values. It indicates how much (on average) predictions deviate from actual values, in the same units as the output (e.g., USD, km/h). For example, MAE = 1000 USD means the model is off by 1000 USD on average.\n",
    "\n",
    "    $MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$\n",
    "\n",
    "\n",
    "2. **Mean Squared Error (MSE)**  \n",
    "   The average of squared differences between predicted and actual values. MSE penalizes large errors more heavily, as each difference is squared. It helps capture situations where the model makes significant errors.\n",
    "\n",
    "    $MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE)**  \n",
    "   The square root of MSE. The result is in the same units as the output data (e.g., USD, kg), which makes it easier to interpret and indicates the average magnitude of prediction errors.\n",
    "\n",
    "    $RMSE = \\sqrt{MSE}$\n",
    "\n",
    "\n",
    "4. **R-squared (R²)**  \n",
    "   Indicates what portion of the variance in the dependent variable is explained by the variables included in the model. It is a measure of how well the model fits the data. The closer R² is to 1, the better the fit.\n",
    "\n",
    "    $R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}$\n",
    "\n",
    "\n",
    "5. **Mean Absolute Percentage Error (MAPE)**  \n",
    "   The average percentage difference between predictions and actual values. It reflects the relative size of the error.\n",
    "\n",
    "    $MAPE = \\frac{100\\%}{n} \\sum_{i=1}^{n} \\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **How to compare regression models and classification models among themselves?**\n",
    "\n",
    "### Formula for Ideal Distance\n",
    "The distance from the ideal metrics for a classification model is calculated as:\n",
    "\n",
    "$\n",
    "\\text{Distance} = \\sqrt{\\sum_{i=1}^n (\\text{Metric}_i - \\text{IdealMetric}_i)^2}\n",
    "$\n",
    "Where:\n",
    "- $ \\text{Metric}_i $ is the value of the $ i $-th metric.\n",
    "- $ \\text{IdealMetric}_i $ is the ideal value of the $ i $-th metric (e.g., 1.0 for all metrics).\n",
    "\n",
    "Ideal values are:\n",
    "- Regression: MAE, RMSE, MAPE = 0.0; R² = 1.0\n",
    "- Classification: accuracy, precision, recall, f1_score = 1.0\n",
    "\n",
    "**Comparator Logic**\n",
    "\n",
    "The comparator ($m1 > m2$) returns `True` if $m1$ has a smaller distance to the ideal metrics than $m2$.\n",
    "\n",
    "**Scoring Function**\n",
    "\n",
    "Additionally, there is a get_score function that calculates a weighted score for each model by assigning different weights to metrics. This provides an alternative way to rank models based on customized priorities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **What will the testing process look like?**\n",
    "\n",
    "#### 1. Data Splitting\n",
    "   Split the data into training (70-80%) and test (20-30%) sets. Additionally, cross-validation (k-fold cross-validation) can be used for more reliable results.\n",
    "\n",
    "#### 2. Model Training  \n",
    "   Train the decision tree on the training data. For regression, train the appropriate model (e.g., linear regression, regression tree, etc.).\n",
    "\n",
    "#### 3. Prediction  \n",
    "   Use the test data to generate predictions (regression) or classes (classification).\n",
    "\n",
    "#### 4. **Model Evaluation** \n",
    "\n",
    "##### 4.1 Metrics for Classification - `evaluation\\evaluate_classification.py`\n",
    "- Compute **Accuracy**, **Precision**, **Recall**, and **F1-Score** for the classification model's results.  \n",
    "- For the regression model's results, map predicted values to intervals using the `values_to_class_labels` function. Then compute the same classification metrics (based on assigned classes).  \n",
    "- Compare classification metrics for both models to assess their ability to classify data.\n",
    "\n",
    "##### Visualization of Classification Metrics\n",
    "- **X-axis:** Metric names (Accuracy, Precision, Recall, F1-Score).  \n",
    "- **Y-axis:** Metric values (on a scale from 0 to 1).\n",
    "- **Goal:** Compare Accuracy, Precision, Recall, and F1-Score for the classification and regression models.\n",
    "- Two sets of bars representing results for the classification and regression models. Bars should be clearly labeled and include a legend.\n",
    "\n",
    "##### 4.2 Metrics for Regression - `evaluation\\evaluate_regression.py`\n",
    "- Compute **MSE**, **RMSE**, **MAE**, **MAPE**, and **R²** for the regression model's results.  \n",
    "- For the classification model's results, map intervals to single values (interval midpoints) using the `labels_to_midpoints` function. Then compute the same regression metrics.  \n",
    "- Compare regression metrics for both models to determine which model generates smaller errors.  \n",
    "- Pay special attention to **MAE** and **MAPE**, as they are the most intuitive and practical for interpretation (e.g., average error in USD or percentage).\n",
    "\n",
    "##### Additional Analysis:\n",
    "- Assess whether the magnitude of the error (e.g., **RMSE = 5000 USD**) is acceptable in the given context.  \n",
    "- Analyze whether the differences between the models' results are **practically significant** or only statistically significant.  \n",
    "- Examine the distribution of errors (residuals) as a function of actual values:  \n",
    "  - Are errors random, or do they follow patterns (e.g., larger deviations for higher values)?  \n",
    "  - If patterns exist, consider model adjustments or feature modifications.\n",
    "\n",
    "\n",
    "**Actual vs. Predicted Plot**\n",
    "\n",
    "   - **X-axis:** Values predicted by the model.  \n",
    "   - **Y-axis:** Actual values.  \n",
    "   - **Goal:** Points should align along the diagonal line (\\( y = x \\)) for an ideal model.\n",
    "\n",
    "**Residual Plot**\n",
    "   - **X-axis:** Predicted values.  \n",
    "   - **Y-axis:** Residuals $(y_{\\text{actual}} - y_{\\text{predicted}})$.  \n",
    "   - **Goal:** Residuals should be randomly distributed around zero, without visible patterns.\n",
    "\n",
    "#### 5. **Evaluate Consistency between Models** - `evaluation\\evaluate_consistency.py`\n",
    "\n",
    "##### 5.1 Define Consistency\n",
    "- The **linear regression model** outputs a point value $ p_{\\text{regression}} $.  \n",
    "- The **decision tree model** outputs a price interval $ [l_{\\text{tree}}, u_{\\text{tree}}] $, where:\n",
    "  - $ l_{\\text{tree}} $: lower bound of the interval.  \n",
    "  - $ u_{\\text{tree}} $: upper bound of the interval.  \n",
    "\n",
    "Consistency occurs if:\n",
    "$$\n",
    "  l_{\\text{tree}} \\leq p_{\\text{regression}} \\leq u_{\\text{tree}}\n",
    "$$\n",
    "\n",
    "##### 5.2 Percentage of Consistency\n",
    "- Calculate the percentage of cases in the dataset where $ p_{\\text{regression}} $ falls within the interval $ [l_{\\text{tree}}, u_{\\text{tree}}] $.  \n",
    "\n",
    "Formula:\n",
    "$$\n",
    "  \\text{Consistency Percentage} = \\frac{\\text{Number of Consistent Cases}}{\\text{Total Cases}} \\times 100\\%\n",
    "$$\n",
    "\n",
    "##### 5.3 Distance from the Interval\n",
    "If $ p_{\\text{regression}} $ does not fall within the interval:\n",
    "- If $ p_{\\text{regression}} < l_{\\text{tree}} $, distance $ = l_{\\text{tree}} - p_{\\text{regression}} $.  \n",
    "- If $ p_{\\text{regression}} > u_{\\text{tree}} $, distance $ = p_{\\text{regression}} - u_{\\text{tree}} $.  \n",
    "\n",
    "Average distance from the interval for all inconsistent cases:\n",
    "$$\n",
    "  \\text{Average Distance} = \\frac{\\sum \\text{Distances for Inconsistent Cases}}{\\text{Number of Inconsistent Cases}}\n",
    "$$\n",
    "\n",
    "##### 5.4 Interpretation of Results\n",
    "**High Consistency**: Both models predict similar values, suggesting their agreement.\n",
    "\n",
    "**Low Consistency**: May indicate different fitting of models to the data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
