{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research metod testowych dla drzew decyzyjnych i regresji\n",
    "\n",
    "## 1.1 Metryki dla klasyfikacji (drzewa decyzyjne)\n",
    "\n",
    "1. **Accuracy (dokładność)**  \n",
    "   Procent poprawnie sklasyfikowanych próbek.  \n",
    "\n",
    "    $\\text{Accuracy} = \\frac{\\text{Liczba poprawnych predykcji}}{\\text{Wszystkie próbki}}$\n",
    "\n",
    "\n",
    "2. **Precision (precyzja)**  \n",
    "   Liczba prawidłowych pozytywnych wyników podzielona przez wszystkie wyniki sklasyfikowane jako pozytywne. Jest to część prawidłowych pozytywnych przewidywań wśród wszystkich przewidywań pozytywnych.\n",
    "\n",
    "    $\\text{Precision} = \\frac{TP}{TP + FP}$\n",
    "\n",
    "\n",
    "3. **Recall (czułość, pokrycie)**  \n",
    "   Liczba prawidłowych pozytywnych wyników podzielona przez wszystkie rzeczywiste pozytywne przypadki. Odsetek prawdziwych pozytywnych przypadków, które model poprawnie rozpoznał jako pozytywne.\n",
    "\n",
    "    $\\text{Recall} = \\frac{TP}{TP + FN}$\n",
    "\n",
    "\n",
    "4. **F1-Score**  \n",
    "   Harmoniczna średnia precyzji i czułości. **F1** służy do zbalansowanej oceny modeli w sytuacjach, gdy ważne jest jednoczesne utrzymanie wysokiej precyzji i czułości.\n",
    "\n",
    "    $F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
    "\n",
    "\n",
    "5. **ROC-AUC (Area Under Curve)**  \n",
    "   Pole pod krzywą ROC (Receiver Operating Characteristic), mierzy zdolność modelu do rozróżniania klas.\n",
    "\n",
    "\n",
    "6. **Log Loss (logarytmiczne straty)**  \n",
    "   Miara penalizująca modele za przewidywania z niską pewnością (im wyższa niepewność lub nietrafność, tym wyższa kara).\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Metryki dla regresji\n",
    "\n",
    "1. **Mean Absolute Error (MAE)**  \n",
    "   Średnia wartość bezwzględnych różnic między przewidywanymi a rzeczywistymi wartościami. Wskazuje, o ile (średnio) „pudłujemy” w jednostkach wyjściowych (np. zł, USD, km/h). Na przykład: MAE = 1000 zł oznacza, że model pomyli się średnio o 1000 zł względem faktycznej ceny czy wartości.\n",
    "\n",
    "    $MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$\n",
    "\n",
    "\n",
    "2. **Mean Squared Error (MSE)**  \n",
    "   Średnia kwadratów różnic między przewidywanymi a rzeczywistymi wartościami. MSE zwiększa „karę” za duże błędy, ponieważ każda różnica jest podnoszona do kwadratu, co pomaga wychwycić sytuacje, w których model znacząco się myli.\n",
    "\n",
    "    $MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE)**  \n",
    "   Pierwiastek kwadratowy z MSE. Wynik jest w tych samych jednostkach co dane wyjściowe (np. dolary, złote czy kilogramy), co często ułatwia interpretację oraz wskazuje, o ile (średnio) model „pudłuje” w przewidywaniach.  \n",
    "\n",
    "    $RMSE = \\sqrt{MSE}$\n",
    "\n",
    "\n",
    "4. **R-squared (R²)**  \n",
    "   Informuje o tym, jaka część zmienności (wariancji) zmiennej objaśnianej w próbie pokrywa się z korelacjami ze zmiennymi zawartymi w modelu. Jest on więc miarą stopnia, w jakim model pasuje do próby. Dopasowanie modelu jest tym lepsze, im wartość R² jest bliższa jedności.\n",
    "\n",
    "    $R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}$\n",
    "\n",
    "\n",
    "5. **Mean Absolute Percentage Error (MAPE)**  \n",
    "   Średnia wartość procentowych różnic między przewidywaniami a wartościami rzeczywistymi.Relatywna (względna) wielkość błędu.\n",
    "\n",
    "    $MAPE = \\frac{100\\%}{n} \\sum_{i=1}^{n} \\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Jak będzie wyglądał proces testowy?**\n",
    "\n",
    "1. **Podział danych**  \n",
    "Podzielimy dane na zbiór treningowy (70-80%) i testowy (20-30%). Dodatkowo można wykorzystać walidację krzyżową (k-fold cross-validation), żeby uzyskać bardziej wiarygodny wynik.\n",
    "\n",
    "2. **Trening modelu**  \n",
    "Wytrenujemy drzewo decyzyjne na danych treningowych.  W przypadku regresji, wytrenujemy odpowiedni model (np. regresję liniową, drzewo regresyjne, itp.).\n",
    "\n",
    "3. **Predykcja**  \n",
    "Użyjemy danych testowych, by wygenerować przewidywane wartości (regresja) lub klasy (klasyfikacja).\n",
    "\n",
    "4. **Ocena modelu** \n",
    "\n",
    "4.1 **Metryki dla klasyfikacji**\n",
    "- Obliczamy **Accuracy**, **Precision**, **Recall** i **F1-Score** dla wyników modelu klasyfikacyjnego.  \n",
    "- Dla wyników modelu regresyjnego mapujemy przewidywane wartości na przedziały za pomocy funkcji values_to_class_labels. Następnie obliczamy te same metryki klasyfikacyjne (na podstawie przypisanych klas).  \n",
    "- Porównujemy metryki klasyfikacyjne dla obu modeli, aby ocenić ich zdolność do klasyfikowania danych.\n",
    "\n",
    "Wykres porównania metryk klasyfikacyjnych\n",
    "- **Cel:** Porównanie Accuracy, Precision, Recall i F1-Score dla modeli klasyfikacyjnego i regresyjnego.\n",
    "- **Oś X:** Nazwy metryk (Accuracy, Precision, Recall, F1-Score).  \n",
    "- **Oś Y:** Wartości metryk (w skali od 0 do 1).  \n",
    "- Dwa zestawy słupków reprezentujące wyniki dla modelu klasyfikacyjnego i regresyjnego. Słupki powinny być wyraźnie opisane i oznaczone legendą.\n",
    "\n",
    "4.2 **Metryki dla regresji**\n",
    "- Obliczamy **MSE**, **RMSE**, **MAE**, **MAPE** i **R²** dla wyników modelu regresyjnego.  \n",
    "- Dla wyników modelu klasyfikacyjnego mapujemy przedziały na pojedyncze wartości (środek przedziału) za pomocy funkcji intervals_to_midpoints. Następnie obliczamy te same metryki regresji.  \n",
    "- Porównujemy metryki regresyjne dla obu modeli, aby ocenić, który model generuje mniejsze błędy.  \n",
    "- Szczególną uwagę zwracamy na **MAE** i **MAPE**, ponieważ są one najbardziej intuicyjne i praktyczne w interpretacji (np. średni błąd w złotówkach lub procentach).  \n",
    "\n",
    "Dodatkowa analiza:\n",
    "- Oceniamy, czy wielkość błędu (np. **RMSE = 5000 zł**) jest akceptowalna w danym kontekście.  \n",
    "- Analizujemy, czy różnice między wynikami modeli są **praktycznie istotne**, czy tylko statystyczne.  \n",
    "- Sprawdzamy rozkład błędów (reszt) w funkcji wartości rzeczywistych:  \n",
    "  - Czy błędy są losowe, czy mają wzorce (np. większe odchylenia dla wyższych wartości)?  \n",
    "  - W przypadku wzorców rozważamy korektę modelu lub modyfikację cech.\n",
    "\n",
    "\n",
    "**Wykres rzeczywiste vs. przewidywane (Actual vs. Predicted)**\n",
    "\n",
    "   - **Oś X:** wartości przewidywane przez model.  \n",
    "   - **Oś Y:** rzeczywiste wartości.  \n",
    "   - Jeśli model jest idealny, punkty powinny leżeć wzdłuż przekątnej (linii $ y = x $).\n",
    "\n",
    "**Wykres reszt (Residual Plot)**\n",
    "   - **Oś X:** Przewidywane wartości.  \n",
    "   - **Oś Y:** Reszty $(y_{\\text{rzeczywiste}} - y_{\\text{przewidywane}})$.\n",
    "   - **Cel:** Reszty powinny rozkładać się losowo wokół zera, bez widocznych wzorców.  \n",
    "   - **Dodatkowa korzyść:** Ułatwia identyfikację problemów, takich jak niedoszacowanie lub przeszacowanie w określonych zakresach wartości.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
